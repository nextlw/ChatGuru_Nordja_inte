/// HybridAIService: Sistema Incremental com Fallback OpenAI ‚Üí Vertex AI
///
/// ARQUITETURA INCREMENTAL:
/// - Sistema OpenAI atual permanece 100% INTACTO
/// - Vertex AI √© uma EXTENS√ÉO OPCIONAL
/// - Fallback autom√°tico: Vertex AI falha ‚Üí OpenAI (garantido)
/// - Feature flags para ativar/desativar componentes
/// - Zero breaking changes no sistema atual
///
/// ESTRAT√âGIA DE SEGURAN√áA:
/// 1. OpenAI Service √© a BASE CONFI√ÅVEL (sempre funciona)
/// 2. Vertex AI √© EXPERIMENTAL (pode ser desabilitado)
/// 3. Configura√ß√£o via TOML define comportamento
/// 4. Logs detalhados para debugging

use crate::utils::{AppError, AppResult};
use crate::utils::logging::*;
use crate::services::openai::{OpenAIService, OpenAIClassification};
use crate::services::vertex::VertexAIService;
use crate::config::Settings;
use serde::{Deserialize, Serialize};

/// Configura√ß√£o do Hybrid AI Service
#[derive(Debug, Clone)]
pub struct HybridAIConfig {
    /// Usar Vertex AI como primeira op√ß√£o (se dispon√≠vel)
    pub use_vertex_primary: bool,
    
    /// Vertex AI est√° habilitado no sistema
    pub vertex_enabled: bool,
    
    /// Timeout para Vertex AI (ms) antes de fallback
    pub vertex_timeout_ms: u64,
    
    /// Sempre usar OpenAI como fallback
    pub openai_fallback_enabled: bool,
    
    /// Log detalhado de decis√µes de AI
    pub verbose_logging: bool,
}

impl Default for HybridAIConfig {
    fn default() -> Self {
        Self {
            use_vertex_primary: false, // Por padr√£o, usar OpenAI
            vertex_enabled: false,     // Vertex desabilitado por padr√£o
            vertex_timeout_ms: 15000,  // 15s timeout
            openai_fallback_enabled: true, // Sempre garantir fallback
            verbose_logging: true,     // Logs detalhados por padr√£o
        }
    }
}

impl HybridAIConfig {
    /// Cria configura√ß√£o a partir das settings do sistema
    pub fn from_settings(settings: &Settings) -> Self {
        let vertex_enabled = settings.vertex
            .as_ref()
            .map(|v| v.enabled)
            .unwrap_or(false);
        
        let ai_settings = settings.ai.as_ref();
        
        Self {
            use_vertex_primary: vertex_enabled, // Se Vertex habilitado, usar como prim√°rio
            vertex_enabled,
            vertex_timeout_ms: settings.vertex
                .as_ref()
                .map(|v| v.timeout_seconds * 1000)
                .unwrap_or(15000),
            openai_fallback_enabled: true, // SEMPRE garantir fallback OpenAI
            verbose_logging: ai_settings
                .map(|ai| ai.enabled)
                .unwrap_or(true),
        }
    }
}

/// Resultado unificado de classifica√ß√£o AI
/// 
/// Mant√©m compatibilidade 100% com OpenAIClassification existente
/// para n√£o quebrar nenhum c√≥digo atual
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HybridAIResult {
    /// Classifica√ß√£o no formato OpenAI (compatibilidade garantida)
    pub classification: OpenAIClassification,
    
    /// Metadados sobre qual servi√ßo foi usado
    pub metadata: AIProcessingMetadata,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AIProcessingMetadata {
    /// Servi√ßo que foi usado para classifica√ß√£o
    pub service_used: AIServiceType,
    
    /// Se houve fallback
    pub fallback_occurred: bool,
    
    /// Tempo de processamento em ms
    pub processing_time_ms: u64,
    
    /// Mensagem de debug
    pub debug_message: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum AIServiceType {
    OpenAI,
    VertexAI,
}

/// HybridAIService: Orquestrador inteligente entre OpenAI e Vertex AI
#[derive(Clone)]
pub struct HybridAIService {
    /// OpenAI Service (BASE CONFI√ÅVEL - sempre inicializado)
    openai_service: OpenAIService,
    
    /// Vertex AI Service (OPCIONAL - pode ser None)
    vertex_service: Option<VertexAIService>,
    
    /// Configura√ß√£o do hybrid service
    config: HybridAIConfig,
}

impl HybridAIService {
    /// Cria novo HybridAIService (incrementa sistema atual)
    ///
    /// GARANTIAS DE SEGURAN√áA:
    /// - OpenAI Service sempre inicializado (sistema atual)
    /// - Vertex AI opcional (pode falhar sem afetar funcionamento)
    /// - Se Vertex AI falha na inicializa√ß√£o, sistema continua s√≥ com OpenAI
    pub async fn new(settings: &Settings) -> AppResult<Self> {
        let config = HybridAIConfig::from_settings(settings);
        
        if config.verbose_logging {
            log_info("üîÑ Inicializando HybridAIService (sistema incremental)");
            log_info(&format!("üìã Configura√ß√£o: vertex_enabled={}, use_vertex_primary={}, fallback_enabled={}", 
                config.vertex_enabled, config.use_vertex_primary, config.openai_fallback_enabled));
        }
        
        // 1. SEMPRE inicializar OpenAI Service (BASE CONFI√ÅVEL)
        let openai_service = match OpenAIService::new(None).await {
            Some(service) => {
                if config.verbose_logging {
                    log_info("‚úÖ OpenAI Service inicializado com sucesso (sistema base)");
                }
                service
            }
            None => {
                log_error("‚ùå FALHA CR√çTICA: N√£o foi poss√≠vel inicializar OpenAI Service");
                return Err(AppError::InternalError(
                    "OpenAI Service √© obrigat√≥rio para funcionamento do sistema".to_string()
                ));
            }
        };
        
        // 2. OPCIONALMENTE inicializar Vertex AI Service (EXPERIMENTAL)
        let vertex_service = if config.vertex_enabled {
            if config.verbose_logging {
                log_info("üöÄ Tentando inicializar Vertex AI Service (experimental)...");
            }
            
            let vertex_settings = settings.vertex.as_ref().unwrap(); // Safe unwrap, j√° verificamos enabled
            
            match VertexAIService::new(
                vertex_settings.project_id.clone(),
                vertex_settings.location.clone(),
                vertex_settings.model.clone(),
            ).await {
                Ok(service) => {
                    if config.verbose_logging {
                        log_info("‚úÖ Vertex AI Service inicializado com sucesso");
                    }
                    Some(service)
                }
                Err(e) => {
                    log_warning(&format!("‚ö†Ô∏è Falha ao inicializar Vertex AI Service: {}. Continuando apenas com OpenAI.", e));
                    None
                }
            }
        } else {
            if config.verbose_logging {
                log_info("‚è≠Ô∏è Vertex AI desabilitado na configura√ß√£o, usando apenas OpenAI");
            }
            None
        };
        
        if config.verbose_logging {
            let status = match (&vertex_service, config.vertex_enabled) {
                (Some(_), true) => "OpenAI + Vertex AI dispon√≠veis",
                (None, true) => "Apenas OpenAI (Vertex AI falhou)",
                (None, false) => "Apenas OpenAI (Vertex AI desabilitado)",
                (Some(_), false) => "Estado inconsistente", // N√£o deveria acontecer
            };
            log_info(&format!("üéØ HybridAIService inicializado: {}", status));
        }
        
        Ok(Self {
            openai_service,
            vertex_service,
            config,
        })
    }
    
    /// Cria HybridAIService a partir do AppState (para integra√ß√£o com worker)
    pub async fn from_app_state(state: &std::sync::Arc<crate::AppState>) -> AppResult<Self> {
        log_info("üèóÔ∏è Criando HybridAIService a partir do AppState");
        Self::new(&state.settings).await
    }
    
    /// Classifica atividade com fallback autom√°tico
    ///
    /// FLUXO INCREMENTAL:
    /// 1. Se Vertex AI dispon√≠vel e habilitado ‚Üí tentar Vertex AI primeiro
    /// 2. Se Vertex AI falha OU n√£o dispon√≠vel ‚Üí usar OpenAI (GARANTIDO)
    /// 3. Sistema atual OpenAI nunca √© afetado
    pub async fn classify_activity(&self, context: &str) -> AppResult<HybridAIResult> {
        let start_time = std::time::Instant::now();
        
        if self.config.verbose_logging {
            log_info(&format!("üîç Iniciando classifica√ß√£o h√≠brida para contexto ({} chars)", context.len()));
        }
        
        // ESTRAT√âGIA 1: Tentar Vertex AI se dispon√≠vel e habilitado
        if self.config.use_vertex_primary && self.vertex_service.is_some() {
            if self.config.verbose_logging {
                log_info("üöÄ Tentando classifica√ß√£o com Vertex AI (prim√°rio)");
            }
            
            match self.try_vertex_classification(context).await {
                Ok(classification) => {
                    let processing_time = start_time.elapsed().as_millis() as u64;
                    
                    if self.config.verbose_logging {
                        log_info(&format!("‚úÖ Vertex AI classifica√ß√£o bem-sucedida em {}ms", processing_time));
                    }
                    
                    return Ok(HybridAIResult {
                        classification,
                        metadata: AIProcessingMetadata {
                            service_used: AIServiceType::VertexAI,
                            fallback_occurred: false,
                            processing_time_ms: processing_time,
                            debug_message: "Vertex AI successful".to_string(),
                        },
                    });
                }
                Err(e) => {
                    if self.config.verbose_logging {
                        log_warning(&format!("‚ö†Ô∏è Vertex AI falhou: {}. Fazendo fallback para OpenAI.", e));
                    }
                    // Continua para fallback OpenAI
                }
            }
        }
        
        // ESTRAT√âGIA 2: OpenAI (SISTEMA BASE - sempre funciona)
        if self.config.verbose_logging {
            let reason = if self.config.use_vertex_primary && self.vertex_service.is_some() {
                "fallback ap√≥s falha Vertex AI"
            } else if !self.config.vertex_enabled {
                "Vertex AI desabilitado"
            } else {
                "Vertex AI n√£o dispon√≠vel"
            };
            log_info(&format!("üîÑ Usando OpenAI ({})", reason));
        }
        
        match self.openai_service.classify_activity_fallback(context).await {
            Ok(classification) => {
                let processing_time = start_time.elapsed().as_millis() as u64;
                
                if self.config.verbose_logging {
                    log_info(&format!("‚úÖ OpenAI classifica√ß√£o bem-sucedida em {}ms", processing_time));
                }
                
                Ok(HybridAIResult {
                    classification,
                    metadata: AIProcessingMetadata {
                        service_used: AIServiceType::OpenAI,
                        fallback_occurred: self.config.use_vertex_primary && self.vertex_service.is_some(),
                        processing_time_ms: processing_time,
                        debug_message: "OpenAI successful".to_string(),
                    },
                })
            }
            Err(e) => {
                log_error(&format!("‚ùå FALHA CR√çTICA: OpenAI tamb√©m falhou: {}", e));
                Err(AppError::InternalError(format!(
                    "Todos os servi√ßos AI falharam. OpenAI: {}", e
                )))
            }
        }
    }
    
    /// Tenta classifica√ß√£o com Vertex AI (com timeout)
    async fn try_vertex_classification(&self, context: &str) -> AppResult<OpenAIClassification> {
        let _vertex_service = self.vertex_service.as_ref()
            .ok_or_else(|| AppError::InternalError("Vertex AI service not available".to_string()))?;
        
        // TODO: Implementar convers√£o Vertex AI ‚Üí OpenAIClassification
        // Por enquanto, returna erro para for√ßar fallback para OpenAI
        // Isso ser√° implementado na pr√≥xima fase
        
        let _vertex_prompt = format!(
            "Classifique se a seguinte mensagem representa uma atividade de trabalho v√°lida.\n\nContexto: {}\n\nResponda em JSON com: is_activity, reason, category, sub_categoria",
            context
        );
        
        // Placeholder: Na implementa√ß√£o real, chamaria vertex_service.process_text()
        // e converteria o resultado para OpenAIClassification
        Err(AppError::VertexError("Vertex AI classification not implemented yet".to_string()))
    }
    
    /// Processa m√≠dia com fallback autom√°tico
    ///
    /// COMPATIBILIDADE: Mant√©m mesma interface que OpenAI Service atual
    pub async fn process_media(&self, media_url: &str, media_type: &str) -> AppResult<String> {
        if self.config.verbose_logging {
            log_info(&format!("üìé Processando m√≠dia: {} ({})", media_url, media_type));
        }
        
        // ESTRAT√âGIA 1: Tentar Vertex AI se dispon√≠vel (multimodal nativo)
        if self.config.use_vertex_primary && self.vertex_service.is_some() {
            if self.config.verbose_logging {
                log_info("üöÄ Tentando processamento de m√≠dia com Vertex AI");
            }
            
            if let Ok(result) = self.try_vertex_media_processing(media_url, media_type).await {
                if self.config.verbose_logging {
                    log_info("‚úÖ Vertex AI processamento de m√≠dia bem-sucedido");
                }
                return Ok(result);
            } else if self.config.verbose_logging {
                log_warning("‚ö†Ô∏è Vertex AI m√≠dia falhou, fazendo fallback para OpenAI");
            }
        }
        
        // ESTRAT√âGIA 2: OpenAI (SISTEMA ATUAL - sempre funciona)
        if self.config.verbose_logging {
            log_info("üîÑ Usando OpenAI para processamento de m√≠dia");
        }
        
        if media_type.contains("audio") {
            // √Åudio: usar OpenAI Whisper
            match self.openai_service.download_audio(media_url).await {
                Ok(audio_bytes) => {
                    let extension = media_url
                        .split('.')
                        .last()
                        .and_then(|ext| ext.split('?').next())
                        .unwrap_or("ogg");
                    
                    self.openai_service.transcribe_audio(&audio_bytes, extension).await
                }
                Err(e) => Err(e),
            }
        } else if media_type.contains("image") {
            // Imagem: usar OpenAI Vision
            match self.openai_service.download_image(media_url).await {
                Ok(image_bytes) => {
                    self.openai_service.describe_image(&image_bytes).await
                }
                Err(e) => Err(e),
            }
        } else {
            Err(AppError::InternalError(format!("Tipo de m√≠dia n√£o suportado: {}", media_type)))
        }
    }
    
    /// Tenta processamento de m√≠dia com Vertex AI
    async fn try_vertex_media_processing(&self, _media_url: &str, _media_type: &str) -> AppResult<String> {
        // TODO: Implementar processamento multimodal real
        // Por enquanto, for√ßa fallback para OpenAI
        Err(AppError::VertexError("Vertex AI media processing not implemented yet".to_string()))
    }
    
    /// Retorna status dos servi√ßos dispon√≠veis
    pub fn get_service_status(&self) -> ServiceStatus {
        ServiceStatus {
            openai_available: true, // Sempre true se inicializou
            vertex_available: self.vertex_service.is_some(),
            vertex_enabled: self.config.vertex_enabled,
            current_strategy: if self.config.use_vertex_primary && self.vertex_service.is_some() {
                "Vertex AI primary with OpenAI fallback".to_string()
            } else {
                "OpenAI only".to_string()
            },
        }
    }
    
    /// Testa conectividade com todos os servi√ßos
    pub async fn test_connectivity(&self) -> ConnectivityTestResult {
        let mut result = ConnectivityTestResult {
            openai_status: "unknown".to_string(),
            vertex_status: "unknown".to_string(),
            overall_health: "unknown".to_string(),
        };
        
        // Teste OpenAI
        match self.openai_service.classify_activity_fallback("teste de conectividade").await {
            Ok(_) => {
                result.openai_status = "healthy".to_string();
                if self.config.verbose_logging {
                    log_info("‚úÖ OpenAI conectividade OK");
                }
            }
            Err(e) => {
                result.openai_status = format!("error: {}", e);
                log_error(&format!("‚ùå OpenAI conectividade falhou: {}", e));
            }
        }
        
        // Teste Vertex AI (se dispon√≠vel)
        if let Some(vertex_service) = &self.vertex_service {
            match vertex_service.test_connection().await {
                Ok(_) => {
                    result.vertex_status = "healthy".to_string();
                    if self.config.verbose_logging {
                        log_info("‚úÖ Vertex AI conectividade OK");
                    }
                }
                Err(e) => {
                    result.vertex_status = format!("error: {}", e);
                    log_warning(&format!("‚ö†Ô∏è Vertex AI conectividade falhou: {}", e));
                }
            }
        } else {
            result.vertex_status = "not_available".to_string();
        }
        
        // Status geral
        result.overall_health = if result.openai_status == "healthy" {
            "healthy".to_string() // OpenAI √© suficiente para sa√∫de geral
        } else {
            "degraded".to_string()
        };
        
        result
    }

    /// M√©todos de compatibilidade para integra√ß√£o com worker existente
    ///
    /// Estes m√©todos mant√™m a mesma interface que o worker espera,
    /// mas usam o sistema h√≠brido internamente
    
    /// Classifica atividade retornando apenas OpenAIClassification (compatibilidade)
    pub async fn classify_activity_with_fallback(&self, context: &str) -> AppResult<OpenAIClassification> {
        match self.classify_activity(context).await {
            Ok(hybrid_result) => Ok(hybrid_result.classification),
            Err(e) => Err(e),
        }
    }
    
    /// Processa √°udio com fallback autom√°tico
    pub async fn process_audio_with_fallback(&self, media_url: &str) -> AppResult<String> {
        self.process_media(media_url, "audio").await
    }
    
    /// Processa imagem com fallback autom√°tico
    pub async fn process_image_with_fallback(&self, media_url: &str) -> AppResult<String> {
        self.process_media(media_url, "image").await
    }
}

#[derive(Debug, Serialize)]
pub struct ServiceStatus {
    pub openai_available: bool,
    pub vertex_available: bool,
    pub vertex_enabled: bool,
    pub current_strategy: String,
}

#[derive(Debug, Serialize)]
pub struct ConnectivityTestResult {
    pub openai_status: String,
    pub vertex_status: String,
    pub overall_health: String,
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::config::settings::{
        Settings, ServerSettings, ClickUpSettings, GcpSettings,
        ChatGuruSettings, AISettings, VertexSettings
    };

    /// Cria configura√ß√£o de teste com Vertex AI desabilitado (comportamento padr√£o)
    fn create_test_settings_vertex_disabled() -> Settings {
        Settings {
            server: ServerSettings { host: "0.0.0.0".to_string(), port: 8080 },
            clickup: ClickUpSettings {
                token: "test_token".to_string(),
                list_id: "test_list".to_string(),
                base_url: "https://api.clickup.com".to_string(),
            },
            gcp: GcpSettings {
                project_id: "test-project".to_string(),
                topic_name: "test-topic".to_string(),
                subscription_name: "test-sub".to_string(),
                pubsub_topic: None,
                media_processing_topic: None,
                media_results_topic: None,
                media_results_subscription: None,
            },
            chatguru: ChatGuruSettings {
                webhook_secret: None,
                validate_signature: false,
                legacy_response_mode: None,
                api_token: None,
                api_endpoint: None,
                account_id: None,
                phone_ids: None,
            },
            vertex: None, // Vertex AI desabilitado
            ai: Some(AISettings {
                enabled: true,
                use_hybrid: Some(true),
                prefer_vertex: Some(false),
            }),
            hybrid_ai: None,
        }
    }

    /// Cria configura√ß√£o de teste com Vertex AI habilitado
    fn create_test_settings_vertex_enabled() -> Settings {
        Settings {
            server: ServerSettings { host: "0.0.0.0".to_string(), port: 8080 },
            clickup: ClickUpSettings {
                token: "test_token".to_string(),
                list_id: "test_list".to_string(),
                base_url: "https://api.clickup.com".to_string(),
            },
            gcp: GcpSettings {
                project_id: "test-project".to_string(),
                topic_name: "test-topic".to_string(),
                subscription_name: "test-sub".to_string(),
                pubsub_topic: None,
                media_processing_topic: None,
                media_results_topic: None,
                media_results_subscription: None,
            },
            chatguru: ChatGuruSettings {
                webhook_secret: None,
                validate_signature: false,
                legacy_response_mode: None,
                api_token: None,
                api_endpoint: None,
                account_id: None,
                phone_ids: None,
            },
            vertex: Some(VertexSettings {
                enabled: true,
                project_id: "test-project".to_string(),
                location: "us-central1".to_string(),
                model: Some("gemini-1.5-pro".to_string()),
                timeout_seconds: 30,
                max_media_size_mb: None,
                supported_mime_types: None,
                generation: None,
            }),
            ai: Some(AISettings {
                enabled: true,
                use_hybrid: Some(true),
                prefer_vertex: Some(true),
            }),
            hybrid_ai: None,
        }
    }

    #[test]
    fn test_hybrid_ai_config_default() {
        let config = HybridAIConfig::default();
        
        assert!(!config.use_vertex_primary, "Por padr√£o, deve usar OpenAI como prim√°rio");
        assert!(!config.vertex_enabled, "Por padr√£o, Vertex AI deve estar desabilitado");
        assert!(config.openai_fallback_enabled, "Fallback OpenAI sempre habilitado");
        assert!(config.verbose_logging, "Logs detalhados por padr√£o");
        assert_eq!(config.vertex_timeout_ms, 15000, "Timeout padr√£o 15s");
        
        println!("‚úÖ HybridAIConfig::default() funcionando corretamente");
    }

    #[test]
    fn test_hybrid_ai_config_from_settings_vertex_disabled() {
        let settings = create_test_settings_vertex_disabled();
        let config = HybridAIConfig::from_settings(&settings);
        
        assert!(!config.vertex_enabled, "Vertex AI deve estar desabilitado");
        assert!(!config.use_vertex_primary, "OpenAI deve ser prim√°rio");
        assert!(config.openai_fallback_enabled, "Fallback sempre habilitado");
        
        println!("‚úÖ Configura√ß√£o com Vertex AI desabilitado funcionando");
    }

    #[test]
    fn test_hybrid_ai_config_from_settings_vertex_enabled() {
        let settings = create_test_settings_vertex_enabled();
        let config = HybridAIConfig::from_settings(&settings);
        
        assert!(config.vertex_enabled, "Vertex AI deve estar habilitado");
        assert!(config.use_vertex_primary, "Vertex AI deve ser prim√°rio quando habilitado");
        assert!(config.openai_fallback_enabled, "Fallback sempre habilitado");
        assert_eq!(config.vertex_timeout_ms, 30000, "Timeout configurado: 30s");
        
        println!("‚úÖ Configura√ß√£o com Vertex AI habilitado funcionando");
    }

    #[test]
    fn test_ai_service_type_serialization() {
        use serde_json;
        
        let openai_type = AIServiceType::OpenAI;
        let vertex_type = AIServiceType::VertexAI;
        
        let openai_json = serde_json::to_string(&openai_type).unwrap();
        let vertex_json = serde_json::to_string(&vertex_type).unwrap();
        
        assert_eq!(openai_json, "\"OpenAI\"");
        assert_eq!(vertex_json, "\"VertexAI\"");
        
        println!("‚úÖ Serializa√ß√£o de AIServiceType funcionando");
    }

    #[test]
    fn test_hybrid_ai_result_structure() {
        use crate::services::openai::OpenAIClassification;
        
        let classification = OpenAIClassification {
            reason: "Teste".to_string(),
            is_activity: true,
            category: Some("Log√≠stica".to_string()),
            campanha: Some("WhatsApp".to_string()),
            description: Some("Teste".to_string()),
            space_name: None,
            folder_name: None,
            list_name: None,
            info_1: None,
            info_2: None,
            tipo_atividade: None,
            sub_categoria: Some("Corrida de motoboy".to_string()),
            subtasks: vec![],
            status_back_office: None,
        };
        
        let metadata = AIProcessingMetadata {
            service_used: AIServiceType::OpenAI,
            fallback_occurred: false,
            processing_time_ms: 150,
            debug_message: "OpenAI successful".to_string(),
        };
        
        let result = HybridAIResult {
            classification: classification.clone(),
            metadata,
        };
        
        // Verificar que a estrutura est√° correta
        assert_eq!(result.classification.reason, "Teste");
        assert!(result.classification.is_activity);
        assert_eq!(result.metadata.processing_time_ms, 150);
        assert!(!result.metadata.fallback_occurred);
        
        println!("‚úÖ Estrutura HybridAIResult correta");
    }

    #[test]
    fn test_service_status_structure() {
        let status = ServiceStatus {
            openai_available: true,
            vertex_available: false,
            vertex_enabled: false,
            current_strategy: "OpenAI only".to_string(),
        };
        
        assert!(status.openai_available);
        assert!(!status.vertex_available);
        assert!(!status.vertex_enabled);
        assert_eq!(status.current_strategy, "OpenAI only");
        
        println!("‚úÖ Estrutura ServiceStatus correta");
    }

    #[test]
    fn test_connectivity_test_result_structure() {
        let result = ConnectivityTestResult {
            openai_status: "healthy".to_string(),
            vertex_status: "not_available".to_string(),
            overall_health: "healthy".to_string(),
        };
        
        assert_eq!(result.openai_status, "healthy");
        assert_eq!(result.vertex_status, "not_available");
        assert_eq!(result.overall_health, "healthy");
        
        println!("‚úÖ Estrutura ConnectivityTestResult correta");
    }

    /// Teste de feature flags
    #[test]
    fn test_feature_flag_vertex_disabled_by_default() {
        let settings = create_test_settings_vertex_disabled(); // Vertex = None
        
        let config = HybridAIConfig::from_settings(&settings);
        
        assert!(!config.vertex_enabled, "Vertex AI deve estar desabilitado por padr√£o");
        assert!(!config.use_vertex_primary, "OpenAI deve ser prim√°rio por padr√£o");
        
        println!("‚úÖ Feature flag: Vertex AI desabilitado por padr√£o");
    }

    #[test]
    fn test_feature_flag_vertex_explicitly_disabled() {
        let mut settings = create_test_settings_vertex_disabled();
        settings.vertex = Some(VertexSettings {
            enabled: false, // Explicitamente desabilitado
            project_id: "test".to_string(),
            location: "us-central1".to_string(),
            model: None,
            timeout_seconds: 30,
            max_media_size_mb: None,
            supported_mime_types: None,
            generation: None,
        });
        
        let config = HybridAIConfig::from_settings(&settings);
        
        assert!(!config.vertex_enabled, "Vertex AI deve estar desabilitado quando enabled=false");
        assert!(!config.use_vertex_primary, "OpenAI deve ser prim√°rio quando Vertex desabilitado");
        
        println!("‚úÖ Feature flag: Vertex AI explicitamente desabilitado");
    }

    #[test]
    fn test_feature_flag_vertex_enabled() {
        let settings = create_test_settings_vertex_enabled();
        
        let config = HybridAIConfig::from_settings(&settings);
        
        assert!(config.vertex_enabled, "Vertex AI deve estar habilitado quando enabled=true");
        assert!(config.use_vertex_primary, "Vertex AI deve ser prim√°rio quando habilitado");
        assert!(config.openai_fallback_enabled, "Fallback OpenAI sempre habilitado");
        
        println!("‚úÖ Feature flag: Vertex AI habilitado");
    }
}